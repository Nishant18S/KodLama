{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f86455-e772-40e1-a8e9-70d6736ed0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import textstat\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8116227f-820c-4529-b4dc-6591af646667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textstat\n",
      "  Downloading textstat-0.7.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyphen (from textstat)\n",
      "  Downloading pyphen-0.17.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting cmudict (from textstat)\n",
      "  Downloading cmudict-1.0.32-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\acer\\anaconda3\\lib\\site-packages (from textstat) (75.1.0)\n",
      "Requirement already satisfied: importlib-metadata>=5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from cmudict->textstat) (7.0.1)\n",
      "Requirement already satisfied: importlib-resources>=5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from cmudict->textstat) (6.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\acer\\anaconda3\\lib\\site-packages (from importlib-metadata>=5->cmudict->textstat) (3.17.0)\n",
      "Downloading textstat-0.7.5-py3-none-any.whl (105 kB)\n",
      "Downloading cmudict-1.0.32-py3-none-any.whl (939 kB)\n",
      "   ---------------------------------------- 0.0/939.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/939.4 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 262.1/939.4 kB ? eta -:--:--\n",
      "   --------------------- ---------------- 524.3/939.4 kB 621.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 939.4/939.4 kB 1.1 MB/s eta 0:00:00\n",
      "Downloading pyphen-0.17.2-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.5/2.1 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.8/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 1.0/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.3/2.1 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: pyphen, cmudict, textstat\n",
      "Successfully installed cmudict-1.0.32 pyphen-0.17.2 textstat-0.7.5\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef4c8b14-c786-4b03-9e96-ff4a859bd45e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as 'essay_scoring_dataset.csv'\n",
      "   essay_id                                         essay_text  score  \\\n",
      "0         1  Education is the most powerful weapon which yo...    8.5   \n",
      "1         2  Climate change is evident through rising tempe...    8.8   \n",
      "2         3  Artificial intelligence revolutionizes healthc...    9.2   \n",
      "3         4  Social media has transformed communication, en...    7.1   \n",
      "4         5  The Industrial Revolution began in 18th centur...    8.7   \n",
      "\n",
      "   prompt_id        topic  grade_level  \\\n",
      "0        100    education  high_school   \n",
      "1        101  environment  high_school   \n",
      "2        102   technology  high_school   \n",
      "3        103      society  high_school   \n",
      "4        104      history  high_school   \n",
      "\n",
      "                                       rubric_scores  \n",
      "0  {'content':9, 'organization':8, 'grammar':9, '...  \n",
      "1  {'content':9, 'organization':9, 'grammar':8, '...  \n",
      "2  {'content':10, 'organization':9, 'grammar':9, ...  \n",
      "3  {'content':7, 'organization':7, 'grammar':8, '...  \n",
      "4  {'content':9, 'organization':9, 'grammar':8, '...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Sample essay data with realistic scores and rubric breakdowns\n",
    "data = {\n",
    "    \"essay_id\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"essay_text\": [\n",
    "        \"Education is the most powerful weapon which you can use to change the world, as Nelson Mandela once said. I completely agree because education develops critical thinking and knowledge. Educated societies are more prosperous and peaceful. While experience is valuable, education provides the foundation for personal and societal growth.\",\n",
    "        \"Climate change is evident through rising temperatures, melting ice caps, and extreme weather. Human activities like burning fossil fuels are the primary cause. We must transition to renewable energy, reforest areas, and reduce waste to mitigate these effects before it's too late for future generations.\",\n",
    "        \"Artificial intelligence revolutionizes healthcare through improved diagnostics, enables personalized education, and enhances business operations. However, it raises concerns about job displacement and privacy. We need ethical guidelines to ensure AI benefits society while minimizing risks of misuse.\",\n",
    "        \"Social media has transformed communication, enabling instant global connections but reducing face-to-face interactions. While it facilitates idea sharing, issues like cyberbullying and addiction persist. Balanced usage is key to harnessing its benefits without negative consequences.\",\n",
    "        \"The Industrial Revolution began in 18th century Britain, introducing mechanized production that spurred urbanization. While it boosted economic output, it also created poor working conditions and environmental pollution. Its technological innovations still influence modern manufacturing processes.\",\n",
    "        \"Regular exercise strengthens cardiovascular health, builds muscle, and reduces stress. I incorporate 30 minutes of activity daily through walking or sports. Finding enjoyable physical activities is crucial for maintaining long-term fitness and mental wellbeing.\",\n",
    "        \"Reading books enhances imagination and vocabulary more than watching movies. Books allow personal interpretation of characters and settings, while films often omit key details. Reading develops critical thinking and creativity that visual media cannot replicate.\",\n",
    "        \"Space exploration drives scientific discovery and technological innovation with Earth applications. Collaborative projects like the ISS demonstrate peaceful international cooperation. While costly, space research often provides solutions to terrestrial challenges, making it a worthwhile investment.\",\n",
    "        \"School uniforms promote equality by reducing socioeconomic disparities visible through clothing. They minimize distractions, enhance safety, and foster school identity. Students can express individuality through accessories and achievements rather than fashion choices.\",\n",
    "        \"Part-time jobs teach teenagers responsibility, time management, and financial literacy. Earning their own money provides practical budgeting experience. With proper scheduling, work complements rather than conflicts with academic responsibilities.\"\n",
    "    ],\n",
    "    \"score\": [8.5, 8.8, 9.2, 7.1, 8.7, 7.5, 8.9, 9.0, 7.8, 8.2],\n",
    "    \"grade_level\": [\"high_school\"]*5 + [\"middle_school\"]*2 + [\"college\"]*3,\n",
    "    \"topic\": [\"education\", \"environment\", \"technology\", \"society\", \"history\", \n",
    "             \"health\", \"literature\", \"science\", \"education\", \"economics\"],\n",
    "    \"rubric_scores\": [\n",
    "        \"{'content':9, 'organization':8, 'grammar':9, 'vocabulary':8}\",\n",
    "        \"{'content':9, 'organization':9, 'grammar':8, 'vocabulary':9}\",\n",
    "        \"{'content':10, 'organization':9, 'grammar':9, 'vocabulary':9}\",\n",
    "        \"{'content':7, 'organization':7, 'grammar':8, 'vocabulary':7}\",\n",
    "        \"{'content':9, 'organization':9, 'grammar':8, 'vocabulary':9}\",\n",
    "        \"{'content':7, 'organization':8, 'grammar':8, 'vocabulary':7}\",\n",
    "        \"{'content':9, 'organization':9, 'grammar':9, 'vocabulary':9}\",\n",
    "        \"{'content':9, 'organization':9, 'grammar':9, 'vocabulary':9}\",\n",
    "        \"{'content':8, 'organization':8, 'grammar':8, 'vocabulary':7}\",\n",
    "        \"{'content':8, 'organization':8, 'grammar':9, 'vocabulary':8}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add prompt_id (mapping topics to IDs)\n",
    "topic_to_prompt = {topic: idx+100 for idx, topic in enumerate(df['topic'].unique())}\n",
    "df['prompt_id'] = df['topic'].map(topic_to_prompt)\n",
    "\n",
    "# Reorder columns\n",
    "df = df[['essay_id', 'essay_text', 'score', 'prompt_id', 'topic', 'grade_level', 'rubric_scores']]\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('essay_scoring_dataset.csv', index=False)\n",
    "\n",
    "print(\"Dataset saved as 'essay_scoring_dataset.csv'\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7160e810-bd29-4f38-be7f-197213d70a35",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\acer/nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[0;32m     28\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124messay_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[1;32m---> 29\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4791\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4796\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4797\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4798\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4799\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4800\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4915\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4916\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4917\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4918\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4919\u001b[0m         func,\n\u001b[0;32m   4920\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4921\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4922\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4923\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4924\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1501\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1505\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1506\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1507\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1508\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1509\u001b[0m )\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1512\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1513\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1741\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1746\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1747\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2972\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_text\u001b[39m(text):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# Tokenization\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Remove stopwords\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\acer/nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\acer\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# Load your essay dataset (replace with your data)\n",
    "# Expected columns: 'essay_id', 'essay_text', 'score'\n",
    "df = pd.read_csv('essay_scoring_dataset.csv')\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Preprocessing pipeline\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_text'] = df['essay_text'].apply(clean_text)\n",
    "df['processed_text'] = df['cleaned_text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b7dca-58e1-4bab-bd7b-1673d6663dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text):\n",
    "    features = {}\n",
    "    \n",
    "    # Basic statistics\n",
    "    features['char_count'] = len(text)\n",
    "    features['word_count'] = len(text.split())\n",
    "    features['sentence_count'] = textstat.sentence_count(text)\n",
    "    features['avg_word_length'] = features['char_count'] / features['word_count'] if features['word_count'] > 0 else 0\n",
    "    features['avg_sentence_length'] = features['word_count'] / features['sentence_count'] if features['sentence_count'] > 0 else 0\n",
    "    \n",
    "    # Readability metrics\n",
    "    features['flesch_reading_ease'] = textstat.flesch_reading_ease(text)\n",
    "    features['smog_index'] = textstat.smog_index(text)\n",
    "    features['coleman_liau_index'] = textstat.coleman_liau_index(text)\n",
    "    \n",
    "    # Vocabulary diversity\n",
    "    words = text.split()\n",
    "    unique_words = set(words)\n",
    "    features['lexical_diversity'] = len(unique_words) / len(words) if len(words) > 0 else 0\n",
    "    \n",
    "    # Grammar and style (simplified)\n",
    "    features['pronoun_count'] = len([word for word in words if word in ['i', 'you', 'he', 'she', 'it', 'we', 'they']])\n",
    "    features['preposition_count'] = len([word for word in words if word in ['in', 'on', 'at', 'by', 'for', 'with']])\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for all essays\n",
    "feature_list = []\n",
    "for text in df['cleaned_text']:\n",
    "    feature_list.append(extract_features(text))\n",
    "    \n",
    "features_df = pd.DataFrame(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721de5cb-e6f1-4f28-b3b2-f83f72764abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_bert_embeddings(text, max_length=512):\n",
    "    # Tokenize text\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length, padding='max_length')\n",
    "    \n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Use mean of last hidden states as document embedding\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    doc_embedding = torch.mean(last_hidden_states, dim=1).squeeze().numpy()\n",
    "    \n",
    "    return doc_embedding\n",
    "\n",
    "# Get BERT embeddings (this may take a while)\n",
    "bert_embeddings = np.array([get_bert_embeddings(text) for text in df['cleaned_text']])\n",
    "bert_columns = [f'bert_{i}' for i in range(bert_embeddings.shape[1])]\n",
    "bert_df = pd.DataFrame(bert_embeddings, columns=bert_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024bf00a-d010-44f8-ba5a-11aa755fefea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features\n",
    "X = pd.concat([features_df, bert_df], axis=1)\n",
    "y = df['score']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed38760-6a54-4b31-ba6d-5faa0b4d0b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Gradient Boosting Model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble prediction\n",
    "def ensemble_predict(X):\n",
    "    rf_pred = rf_model.predict(X)\n",
    "    gb_pred = gb_model.predict(X)\n",
    "    return (rf_pred + gb_pred) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ec9416-e27b-4561-9f53-71040de164a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "y_pred = ensemble_predict(X_test)\n",
    "\n",
    "# Evaluation metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "pearson_r = stats.pearsonr(y_test, y_pred)[0]\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"Mean Absolute Error: {mae:.4f}\")\n",
    "print(f\"Pearson Correlation: {pearson_r:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562149dc-fe3d-471b-8a20-142a27e4e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def generate_feedback(essay_text):\n",
    "    # Preprocess\n",
    "    cleaned_text = clean_text(essay_text)\n",
    "    processed_text = preprocess_text(cleaned_text)\n",
    "    \n",
    "    # Extract features\n",
    "    features = extract_features(cleaned_text)\n",
    "    \n",
    "    # Get BERT embedding\n",
    "    bert_embedding = get_bert_embeddings(cleaned_text)\n",
    "    \n",
    "    # Combine features\n",
    "    features_df = pd.DataFrame([features])\n",
    "    bert_df = pd.DataFrame([bert_embedding], columns=bert_columns)\n",
    "    X = pd.concat([features_df, bert_df], axis=1)\n",
    "    \n",
    "    # Predict score\n",
    "    score = ensemble_predict(X)[0]\n",
    "    \n",
    "    # Generate feedback\n",
    "    feedback = defaultdict(list)\n",
    "    \n",
    "    # Word count feedback\n",
    "    if features['word_count'] < 200:\n",
    "        feedback['Structure'].append(\"Your essay is too short. Try to expand your ideas with more details and examples.\")\n",
    "    elif features['word_count'] > 1000:\n",
    "        feedback['Structure'].append(\"Your essay is too long. Try to be more concise and focus on your main points.\")\n",
    "    \n",
    "    # Readability feedback\n",
    "    if features['flesch_reading_ease'] < 60:\n",
    "        feedback['Style'].append(\"Your writing may be too complex. Consider using simpler sentence structures.\")\n",
    "    \n",
    "    # Vocabulary feedback\n",
    "    if features['lexical_diversity'] < 0.5:\n",
    "        feedback['Vocabulary'].append(\"Try to use more varied vocabulary. Consider using synonyms and different expressions.\")\n",
    "    \n",
    "    # Grammar feedback (simplified)\n",
    "    if features['pronoun_count'] / features['word_count'] > 0.15:\n",
    "        feedback['Grammar'].append(\"You may be overusing pronouns. Try to vary your sentence structure.\")\n",
    "    \n",
    "    # Convert feedback to string\n",
    "    feedback_str = f\"Predicted Score: {score:.1f}/10\\n\\n\"\n",
    "    for category, comments in feedback.items():\n",
    "        feedback_str += f\"{category}:\\n\"\n",
    "        for comment in comments:\n",
    "            feedback_str += f\"- {comment}\\n\"\n",
    "        feedback_str += \"\\n\"\n",
    "    \n",
    "    if len(feedback) == 0:\n",
    "        feedback_str += \"Good job! Your essay meets basic requirements. Keep up the good work!\"\n",
    "    \n",
    "    return feedback_str\n",
    "\n",
    "# Example usage\n",
    "sample_essay = \"\"\"\n",
    "The importance of education cannot be overstated. Education is key to personal development. \n",
    "Education helps people get better jobs. Education makes society better. Everyone should go to school.\n",
    "\"\"\"\n",
    "\n",
    "print(generate_feedback(sample_essay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2b3b8-a3dd-497f-a2a2-524caa73e312",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Save models\n",
    "joblib.dump(rf_model, 'random_forest_model.pkl')\n",
    "joblib.dump(gb_model, 'gradient_boosting_model.pkl')\n",
    "\n",
    "# Save BERT tokenizer and model\n",
    "model.save_pretrained('./bert_model/')\n",
    "tokenizer.save_pretrained('./bert_model/')\n",
    "\n",
    "# Save feature columns for reference\n",
    "with open('feature_columns.pkl', 'wb') as f:\n",
    "    pickle.dump(list(X.columns), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c46c81-b494-4657-83a3-c9e49e878cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
